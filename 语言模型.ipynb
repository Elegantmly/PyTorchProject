{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语言模型\n",
    "\n",
    "学习目标\n",
    "\n",
    "- 学习语言模型，以及如何训练一个语言模型\n",
    "- 学习torchtex的基本使用方法\n",
    "    - 构建vocabulary\n",
    "    - word to index 和 index to word\n",
    "- 学习torch.nn的一些基本模型\n",
    "    - Linear\n",
    "    - RNN\n",
    "    - LSTM\n",
    "    - GRU\n",
    "- RNN的训练技巧\n",
    "    - Gradient Clipping\n",
    "- 如何保存和读取模型\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![什么是语言模型](images/4.png)\n",
    "\n",
    "![链式法则](images/5.png)\n",
    "\n",
    "Markov假设：每个单词只跟它之前n个单词有关\n",
    "![Markov假设](images/6.png)\n",
    "\n",
    "语言模型的评价：\n",
    "句子概率越大，语言模型越好，迷惑度越小。\n",
    "负号：概率非常小，往往是0.00000...，取负的次方可以变成一个比较大的数字。\n",
    "1/N：句子的长度是不一样的，长的句子自然概率就小，取1/N次方后可以normalize，把语句的长度的因素取消。\n",
    "这样就会拿到一个正的数字，PP越低，表示这个语句比较符合模型的预期。\n",
    "![Perplexity](images/7.png)\n",
    "\n",
    "用神经网络来拟合概率p\n",
    "![](images/8.png)\n",
    "\n",
    "循环神经网络\n",
    "![](images/9.png)\n",
    "相当于不断做线性变换，拿到h_t后，要映射到整个单词的空间上去。h_t\n",
    "可能是300维，y_t是要展示在整个vocab_size，假设是50000维，所以W_s\n",
    "是一个50000x300的向量\n",
    "![](images/10.png)\n",
    "\n",
    "需要在每一个单词上做cross Entropy的计算，y帽是预测结果，y是ground truth\n",
    "![](images/11.png)\n",
    "\n",
    "梯度消失和爆炸\n",
    "![](images/12.png)\n",
    "梯度太大，往下卡，卡到低于threshold的位置\n",
    "梯度太小，用relu函数\n",
    "![](images/13.png)\n",
    "\n",
    "LSTM\n",
    "![](images/14.png)\n",
    "\n",
    "![](images/15.png)\n",
    "![](images/16.png)\n",
    "![](images/17.png)\n",
    "![](images/18.png)\n",
    "forget gate\n",
    "![](images/19.png)\n",
    "i是input gate\n",
    "![](images/20.png)\n",
    "![](images/21.png)\n",
    "output gate\n",
    "![](images/22.png)\n",
    "公式，后面的输出对前面求导时，梯度并没有完全小时\n",
    "![](images/23.png)\n",
    "更常用的版本，pytorch中使用的\n",
    "![](images/24.png)\n",
    "GRU\n",
    "![](images/25.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "循环神经网络（Recurrent Neural Network，RNN）是一种用于处理序列数据的神经网络。相比一般的神经网络来说，他能够处理序列变化的数据。比如某个单词的意思会因为上文提到的内容不同而有不同的含义，RNN就能够很好地解决这类问题。\n",
    "![](images/26.png)\n",
    "x为当前状态下数据的输入，h表示接收到的上一个节点的输入。\n",
    "y为当前节点状态下的输出，而h’为传递到下一个节点的输出。\n",
    "从公式可以看出，h'和x、h都相关。y通常使用h'投入到一个线形层（主要是进行维度映射）然后使用softmax进行分类得到所需要的数据。\n",
    "通过序列形式的输入，可以得到如下形式的RNN：\n",
    "![](images/27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "#### 什么是LSTM\n",
    "长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。\n",
    "\n",
    "LSTM结构和普通RNN的主要输入输出区别如下所示。\n",
    "![](images/28.png)\n",
    "相比RNN只有一个传递状态h_t，LSTM有两个传输状态，一个c_t（cell state)，和一个h_t(hidden state)。（RNN中的h_t相当于LSTM中的c_t）\n",
    "其中对于传递下去的c_t改变的很慢，通常输出的c_t是上一个状态传过来的c_(t-1)加上一些数值。\n",
    "\n",
    "#### 深入LSTM结构\n",
    "首先使用LSTM的当前输入x_t和上一个状态传递下来的h_(t-1)拼接训练得到四个状态。\n",
    "![](images/29.png)\n",
    "其中，z_f,z_i,z_o是由拼接向量乘以权重矩阵之后，再通过一个sigmoid激活函数转换成0到1之间的数值，来作为一种门控状态。而z则是将结果通过一个tanh激活函数转换到（-1，1）之间的值（这里使用tanh是因为这里是将其作为输入数据，而不是门控信号）。\n",
    "![](images/30.png)\n",
    "LSTM内部主要有三个阶段\n",
    "- 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会“忘记不重要的，记住重要的”。具体来说是通过计算得到的z_f（f表示forget）来作为忘记门控，来控制上一个状态的c_(t-1)哪些需要留哪些需要忘。\n",
    "- 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入x_t进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的z表示，而选择的门控信号则是由z_i(i代表information)来进行控制。\n",
    "    - 将上面两步得到的结果相加，即可得到传输给下一个状态的c_t。也就是上图中的第一个公式。\n",
    "- 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过z_o来进心控制的。并且还对上一阶段得到的c_o进行了放缩(通过一个tanh激活函数来进行变化)。\n",
    "\n",
    "与普通RNN类似，输出y_t往往最终也是通过h_t变化得到的。\n",
    "\n",
    "#### 总结\n",
    "以上，就是LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。\n",
    "\n",
    "但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU\n",
    "GRU是RNN的一种，和LSTM一样，是为了解决长期记忆和反向传播中的梯度等问题而提出来的。\n",
    "GRU和LSTM在很多情况下实际表现上相差无几，那么为什么要使用较新的GRU（2014）而不是相对经受了更多考验的LSTM（1997）呢？原因就是GRU的实验效果与LSTM相似，但是更容易计算。相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。\n",
    "#### GRU的输入输出结构\n",
    "GRU的输入输出结构和普通的RNN是一样的。\n",
    "有一个当前的输入x_t，和上一个节点传递下来的隐状态h_(t-1)，这个隐状态包含了之前节点的相关信息。\n",
    "结合x_t和h_(t-1)，GRU就会得到当前隐藏节点的输出y_t和传递给下一个节点的隐状态h_t。\n",
    "![GRU的输入输出结构](images/31.png)\n",
    "\n",
    "#### GRU的内部结构\n",
    "首先，我们先通过上一个传输下来的状态h_(t-1)和当前节点的输入x_t来获取两个门控状态。如下图所以，其中r控制重置的门控（reset gate），z为控制更新的门控（update gate）。\n",
    "![GRU的输入输出结构](images/32.png)\n",
    "得到门控信号之后，首先使用重置门来得到“重置”之后的数据$h^{t-1}$$'$=$h^{t-1}\\bigodot r$,再将h_(t-1)'与输入x_t进行拼接，再通过一个tanh激活函数来将数据放缩到(-1,1)的范围内。即得到如下所示的h'。\n",
    "![](images/33.png)\n",
    "这里的h'主要是包含了当前输入的x_t数据，有针对性地对h'添加到当前的隐藏状态，相当于记忆了当前时刻的状态，类似于LSTM地选择记忆阶段。\n",
    "\n",
    "![](images/33.png)\n",
    "\n",
    "最后介绍GRU最关键的一个步骤，可以称之为“更新记忆”阶段。\n",
    "在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控z（update gate）。\n",
    "\n",
    "更新表达式：$h^t$ = $(1-z) \\bigodot h^{t-1} + z \\bigodot h'$\n",
    "\n",
    "首先再次强调一下，门控信号（z）的范围为0-1，门控信号越接近1，代表“记忆”下来的数据越多；而越接近0，则代表“遗忘”的越多。\n",
    "\n",
    "**GRU很聪明的一点就在于，我们使用了同一个门控z就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。**\n",
    "\n",
    "- $(1-z) \\bigodot h^{t-1}$ ：表示对原本隐藏状态的选择性“遗忘”。这里的1-z可以想象成遗忘门，忘记h_(t-1)维度中一些不重要的信息。\n",
    "- $z \\bigodot h'$ ：表示对包含当前节点信息的h’进行选择性“记忆”。与上面类似，这里的（1-z）同理会忘记h’维度中的一些不重要的信息。或者，这里我们更应当看作是对h’维度中的某些信息进行选择。\n",
    "- $h^t = (1-z) \\bigodot h^{t-1} + z \\bigodot h' $：结合上述，这一步的操作就是忘记传递下来的h_(t-1)中的某些维度信息，并加入当前节点输入的某些维度信息。\n",
    ">可以看到，这里的遗忘z和选择(1-z）是联动的，也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重（z），我们就会使用包含当前输入的h'中所对应的权重进行弥补(1-z），以保持一种“恒定”状态。\n",
    "\n",
    "#### LSTM和GRU的关系\n",
    "在这里，r(reset gate)实际上与他的名字有点不符，我们仅仅用它来获得了h'。\n",
    "那么这里的h'实际上可以看成对应于LSTM中的hidden state；上一个节点传下来的h_(t-1)则对应于LSTM中的cell state。1-z对应的则是LSTM中的z_f forget gate，那么z似乎就可以看成选择门z_i了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们会使用torchtext来创建vocabulary，然后把数据读成batch的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(53113)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 100\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "HIDDEN_SIZE = 100\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们会继续使用上次的text8作为我们的训练、验证和测试数据\n",
    "- TorchText的一个重要概念是Field，它决定了你的数据会如何被处理。我们使用TEXT这个field来处理文本数据。我们的TEXT field有lower =True这个参数，所以所有的单词都会被lowercase。\n",
    "- torchtext提供了LanguageModelingDatset这个class来帮助我们处理语言模型数据集。\n",
    "- build——vocab可以根据我们提供的训练数据集来创建最高频单词的单词表，max_size帮助我们限定单词总量\n",
    "- BPTTIterator可以连续地得到连贯的句子，BPTT的全称是back propagation through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(lower=True)\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(path=\".\",\n",
    "            train = \"text8/text8.train.txt\",validation = \"text8/text8.dev.txt\",\n",
    "            test = \"text8/text8.test.txt\",text_field=TEXT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取频率最高的单词留下来，作为单词表，torchtext做好了这件事\n",
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50002"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为什么我们的单词表有50002个单词而不是50000呢？因为Torch Text给我们增加了两个特殊的token，```<unk>```表示未知的单词，```<pad>```表示padding(补全太短的单词)\n",
    "- 模型的输入是一串文字，模型的输出也是一串文字，他们之间相差一个位置，因为语言模型的目标是根据之前的单词预测下一个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1273"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在要构建一个iterator，在训练和测试集上有这么多的text，希望把它变成一个个的batch\n",
    "# 每个batch里面包含32个句子，用这个32个句子作训练\n",
    "# bptt_len是往回传的长度要有多少\n",
    "# repeat走往一个文件后会不会重复\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "        (train, val, test), batch_size=BATCH_SIZE, device=device,\n",
    "        bptt_len=50, repeat=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.text]:[torch.cuda.LongTensor of size 50x32 (GPU 0)]\n",
       "\t[.target]:[torch.cuda.LongTensor of size 50x32 (GPU 0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text是文本\n",
    "# target是要预测的句子\n",
    "# 50是句子的长度，32是batch_size\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4815,   50,    6,  ..., 9116,   33,    7],\n",
       "        [3143, 2748,  495,  ...,  893,  277,  317],\n",
       "        [  13,    8,  850,  ...,  664,  824, 1602],\n",
       "        ...,\n",
       "        [   8,   34,  522,  ..., 5237,    3,   12],\n",
       "        [3628, 1266,  968,  ...,    3,    2,    6],\n",
       "        [   2,   54,   78,  ...,   12,  185, 3027]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the\n",
      "\n",
      "originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(TEXT.vocab.itos[i] for i in batch.text[:,0].data.cpu()))\n",
    "print()\n",
    "print(\" \".join(TEXT.vocab.itos[i] for i in batch.target[:,0].data.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing\n",
      "\n",
      "of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations\n",
      "1\n",
      "interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or <unk> but rather a harmonious anti authoritarian society in place of what are regarded\n",
      "\n",
      "of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or <unk> but rather a harmonious anti authoritarian society in place of what are regarded as\n",
      "2\n",
      "as authoritarian political structures and coercive economic institutions anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society\n",
      "\n",
      "authoritarian political structures and coercive economic institutions anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society however\n",
      "3\n",
      "however ideas about how an anarchist society might work vary considerably especially with respect to economics there is also disagreement about how a free society might be brought about origins and predecessors kropotkin and others argue that before recorded history human society was organized on anarchist principles most anthropologists follow\n",
      "\n",
      "ideas about how an anarchist society might work vary considerably especially with respect to economics there is also disagreement about how a free society might be brought about origins and predecessors kropotkin and others argue that before recorded history human society was organized on anarchist principles most anthropologists follow kropotkin\n",
      "4\n",
      "kropotkin and engels in believing that hunter gatherer bands were egalitarian and lacked division of labour accumulated wealth or decreed law and had equal access to resources william godwin anarchists including the the anarchy organisation and rothbard find anarchist attitudes in taoism from ancient china kropotkin found similar ideas in\n",
      "\n",
      "and engels in believing that hunter gatherer bands were egalitarian and lacked division of labour accumulated wealth or decreed law and had equal access to resources william godwin anarchists including the the anarchy organisation and rothbard find anarchist attitudes in taoism from ancient china kropotkin found similar ideas in stoic\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    batch = next(it)\n",
    "    print(i)\n",
    "    print(\" \".join(TEXT.vocab.itos[i] for i in batch.text[:,0].data.cpu()))\n",
    "    print()\n",
    "    print(\" \".join(TEXT.vocab.itos[i] for i in batch.target[:,0].data.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "- 继承nn.Module\n",
    "- 初始化函数\n",
    "- forward函数\n",
    "- 其余可以根据模型需要定义相关的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    # 以LSTM为例\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        # 初始化继承的类\n",
    "        super(RNNModel, self).__init__()\n",
    "        # 把每个单词映射成embed_size大小的向量,随机初始化，后面会训练\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # input_size:输入数据的特征维数，通常就是embedding_dim（词向量的维度\n",
    "        # hidden_size:LSTM中隐层的维度\n",
    "        # num_layers: 循环神经网络的层数\n",
    "        # num_directions=2,否则就是1，表示只有1个方向\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
    "        \n",
    "        # 最后输出要是一个50002维的向量，才能被argmax\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, text, hidden):\n",
    "        \n",
    "        # text:seq_length * batch_size\n",
    "        emb = self.embed(text) # seq_length * batch_size * embed_size\n",
    "        output, hidden = self.lstm(emb, hidden) \n",
    "        # output:seq_length * batch_size * hidden_size 把输出的每一个hidden_states都拿出来了\n",
    "        # (lstm可以有若干层，1层的话就是1*）hidden: (1 * batchsize * hidden_size,  1 * batch_size * hidden_size)\n",
    "        # hidden: 最后一个hidden_states\n",
    "        # output[-1]与h_n是相等的，因为output[-1]包含的正是batch_size个句子中每一个句子的最后一个单词的隐藏状态，注意LSTM中的隐藏状态其实就是输出，cell state细胞状态才是LSTM中一直隐藏的，记录着信息\n",
    "        \n",
    "        \n",
    "        # output是三维的，但是做线性变换的时候要求output是两维的\n",
    "        # 把前两个维度拼到一起\n",
    "        # output = output.view(-1, output.shape[2]) # (seq_length * batch_size) * hidden_size \n",
    "        out_vocab = self.linear(output.view(-1, output.shape[2])) # (seq_length * batch_size) * vocab_size\n",
    "        out_vocab = out_vocab.view(output.size(0), output.size(1), out_vocab.size(-1))\n",
    "        \n",
    "        #每一个位置分别预测了哪一个单词\n",
    "        return out_vocab, hidden\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, bsz, requires_grad = True):\n",
    "        # 初始化hidden_state\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros((1, bsz, self.hidden_size), requires_grad=True),\n",
    "                weight.new_zeros((1, bsz, self.hidden_size), requires_grad=True))\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size=len(TEXT.vocab), \n",
    "                 embed_size=EMBEDDING_SIZE,\n",
    "                 hidden_size=HIDDEN_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embed): Embedding(50002, 100)\n",
       "  (lstm): LSTM(100, 100)\n",
       "  (linear): Linear(in_features=100, out_features=50002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    # 把这个h和之前所有的history全部detach掉\n",
    "    # 把这个节点从前面的计算图里截断，等于这个h是复制了原来的值，但没有把历史背下来\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        # h如果是两个，每个都package一下\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# scheduler可以帮助调lr\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval() # 训练模式\n",
    "    total_loss = 0.\n",
    "    total_count = 0.\n",
    "    it = iter(data)\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
    "        for i, batch in enumerate(it):\n",
    "            data, target = batch.text, batch.target\n",
    "\n",
    "            # 确保是全新的hidden，而不是带着一大堆历史的hidden\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            # 因为句子首尾相连，所以可以直接传hidden\n",
    "            output, hidden = model(data, hidden) # backpropgate through all iterations 计算图会非常大而且非常深\n",
    "\n",
    "            loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1)) # batch_size * target_class_dim, batch_size\n",
    "            total_loss = loss.item() * np.multiply(*data.size())\n",
    "            total_count = np.multiply(*data.size())\n",
    "    \n",
    "    loss = total_loss / total_count\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iteration 0 loss 7.050442695617676\n",
      "best model saved to lm.pth\n",
      "epoch 0 iteration 100 loss 6.808845043182373\n",
      "epoch 0 iteration 200 loss 6.653306484222412\n",
      "epoch 0 iteration 300 loss 6.944944381713867\n",
      "epoch 0 iteration 400 loss 6.815479278564453\n",
      "epoch 0 iteration 500 loss 6.660840034484863\n",
      "epoch 0 iteration 600 loss 6.570061206817627\n",
      "epoch 0 iteration 700 loss 6.5757060050964355\n",
      "epoch 0 iteration 800 loss 6.40944766998291\n",
      "epoch 0 iteration 900 loss 6.6818437576293945\n",
      "epoch 0 iteration 1000 loss 6.7329559326171875\n",
      "best model saved to lm.pth\n",
      "epoch 0 iteration 1100 loss 6.416387557983398\n",
      "epoch 0 iteration 1200 loss 6.332454204559326\n",
      "epoch 0 iteration 1300 loss 6.446369171142578\n",
      "epoch 0 iteration 1400 loss 6.128020763397217\n",
      "epoch 0 iteration 1500 loss 6.2821364402771\n",
      "epoch 0 iteration 1600 loss 6.072458267211914\n",
      "epoch 0 iteration 1700 loss 6.366618633270264\n",
      "epoch 0 iteration 1800 loss 6.417938232421875\n",
      "epoch 0 iteration 1900 loss 6.3945441246032715\n",
      "epoch 0 iteration 2000 loss 6.350003719329834\n",
      "best model saved to lm.pth\n",
      "epoch 0 iteration 2100 loss 6.3094611167907715\n",
      "epoch 0 iteration 2200 loss 6.295630931854248\n",
      "epoch 0 iteration 2300 loss 6.2228193283081055\n",
      "epoch 0 iteration 2400 loss 6.239324569702148\n",
      "epoch 0 iteration 2500 loss 6.335003852844238\n",
      "epoch 0 iteration 2600 loss 6.322857856750488\n",
      "epoch 0 iteration 2700 loss 5.9076008796691895\n",
      "epoch 0 iteration 2800 loss 6.289426326751709\n",
      "epoch 0 iteration 2900 loss 6.11578369140625\n",
      "epoch 0 iteration 3000 loss 6.326555252075195\n",
      "best model saved to lm.pth\n",
      "epoch 0 iteration 3100 loss 6.305781364440918\n",
      "epoch 0 iteration 3200 loss 6.215181827545166\n",
      "epoch 0 iteration 3300 loss 6.180981636047363\n",
      "epoch 0 iteration 3400 loss 6.05143928527832\n",
      "epoch 0 iteration 3500 loss 6.213362216949463\n",
      "epoch 0 iteration 3600 loss 6.129461765289307\n",
      "epoch 0 iteration 3700 loss 5.892785549163818\n",
      "epoch 0 iteration 3800 loss 5.935121059417725\n",
      "epoch 0 iteration 3900 loss 6.027873516082764\n",
      "epoch 0 iteration 4000 loss 6.0011305809021\n",
      "best model saved to lm.pth\n",
      "epoch 0 iteration 4100 loss 6.050589084625244\n",
      "epoch 0 iteration 4200 loss 6.02749490737915\n",
      "epoch 0 iteration 4300 loss 6.021569728851318\n",
      "epoch 0 iteration 4400 loss 5.934071063995361\n",
      "epoch 0 iteration 4500 loss 5.896610260009766\n",
      "epoch 0 iteration 4600 loss 5.9339680671691895\n",
      "epoch 0 iteration 4700 loss 6.268228530883789\n",
      "epoch 0 iteration 4800 loss 6.032566070556641\n",
      "epoch 0 iteration 4900 loss 5.751905918121338\n",
      "epoch 0 iteration 5000 loss 5.836798667907715\n",
      "best model saved to lm.pth\n",
      "epoch 0 iteration 5100 loss 5.841296195983887\n",
      "epoch 0 iteration 5200 loss 6.051893711090088\n",
      "epoch 0 iteration 5300 loss 6.106024265289307\n",
      "epoch 0 iteration 5400 loss 5.720950126647949\n",
      "epoch 0 iteration 5500 loss 5.983785629272461\n",
      "epoch 0 iteration 5600 loss 5.868088245391846\n",
      "epoch 0 iteration 5700 loss 5.905835151672363\n",
      "epoch 0 iteration 5800 loss 5.87559700012207\n",
      "epoch 0 iteration 5900 loss 6.0899338722229\n",
      "epoch 0 iteration 6000 loss 5.875882148742676\n",
      "best model saved to lm.pth\n",
      "epoch 0 iteration 6100 loss 5.845910549163818\n",
      "epoch 0 iteration 6200 loss 5.926008701324463\n",
      "epoch 0 iteration 6300 loss 5.956418514251709\n",
      "epoch 0 iteration 6400 loss 5.82943868637085\n",
      "epoch 0 iteration 6500 loss 5.654985427856445\n",
      "epoch 0 iteration 6600 loss 5.976761341094971\n",
      "epoch 0 iteration 6700 loss 5.759096145629883\n",
      "epoch 0 iteration 6800 loss 5.815850734710693\n",
      "epoch 0 iteration 6900 loss 6.0487284660339355\n",
      "epoch 0 iteration 7000 loss 5.805119514465332\n",
      "best model saved to lm.pth\n",
      "epoch 0 iteration 7100 loss 5.785564422607422\n",
      "epoch 0 iteration 7200 loss 5.914753437042236\n",
      "epoch 0 iteration 7300 loss 5.824789524078369\n",
      "epoch 0 iteration 7400 loss 5.876354217529297\n",
      "epoch 0 iteration 7500 loss 5.667079448699951\n",
      "epoch 0 iteration 7600 loss 5.7351250648498535\n",
      "epoch 0 iteration 7700 loss 5.874149322509766\n",
      "epoch 0 iteration 7800 loss 5.78257942199707\n",
      "epoch 0 iteration 7900 loss 5.836392402648926\n",
      "epoch 0 iteration 8000 loss 5.759852409362793\n",
      "best model saved to lm.pth\n",
      "epoch 0 iteration 8100 loss 5.790067672729492\n",
      "epoch 0 iteration 8200 loss 5.5861406326293945\n",
      "epoch 0 iteration 8300 loss 5.497269153594971\n",
      "epoch 0 iteration 8400 loss 5.944177150726318\n",
      "epoch 0 iteration 8500 loss 5.7523345947265625\n",
      "epoch 0 iteration 8600 loss 5.772579193115234\n",
      "epoch 0 iteration 8700 loss 5.6334357261657715\n",
      "epoch 0 iteration 8800 loss 5.692863941192627\n",
      "epoch 0 iteration 8900 loss 6.07144832611084\n",
      "epoch 0 iteration 9000 loss 5.683774471282959\n",
      "best model saved to lm.pth\n",
      "epoch 0 iteration 9100 loss 5.7870354652404785\n",
      "epoch 0 iteration 9200 loss 5.503012180328369\n",
      "epoch 0 iteration 9300 loss 5.538395881652832\n",
      "epoch 0 iteration 9400 loss 5.5782575607299805\n",
      "epoch 0 iteration 9500 loss 5.909580230712891\n",
      "epoch 1 iteration 0 loss 5.8555192947387695\n",
      "best model saved to lm.pth\n",
      "epoch 1 iteration 100 loss 5.803311824798584\n",
      "epoch 1 iteration 200 loss 5.661802768707275\n",
      "epoch 1 iteration 300 loss 6.052133560180664\n",
      "epoch 1 iteration 400 loss 5.7523698806762695\n",
      "epoch 1 iteration 500 loss 5.622992038726807\n",
      "epoch 1 iteration 600 loss 5.6133222579956055\n",
      "epoch 1 iteration 700 loss 5.62152099609375\n",
      "epoch 1 iteration 800 loss 5.571402072906494\n",
      "epoch 1 iteration 900 loss 5.729134559631348\n",
      "epoch 1 iteration 1000 loss 5.687146186828613\n",
      "best model saved to lm.pth\n",
      "epoch 1 iteration 1100 loss 5.64292049407959\n",
      "epoch 1 iteration 1200 loss 5.463135242462158\n",
      "epoch 1 iteration 1300 loss 5.586784839630127\n",
      "epoch 1 iteration 1400 loss 5.287460803985596\n",
      "epoch 1 iteration 1500 loss 5.51688289642334\n",
      "epoch 1 iteration 1600 loss 5.347339630126953\n",
      "epoch 1 iteration 1700 loss 5.571990489959717\n",
      "epoch 1 iteration 1800 loss 5.766930103302002\n",
      "epoch 1 iteration 1900 loss 5.6138739585876465\n",
      "epoch 1 iteration 2000 loss 5.742559432983398\n",
      "best model saved to lm.pth\n",
      "epoch 1 iteration 2100 loss 5.6507134437561035\n",
      "epoch 1 iteration 2200 loss 5.7115092277526855\n",
      "epoch 1 iteration 2300 loss 5.61674690246582\n",
      "epoch 1 iteration 2400 loss 5.56285285949707\n",
      "epoch 1 iteration 2500 loss 5.704470634460449\n",
      "epoch 1 iteration 2600 loss 5.7365827560424805\n",
      "epoch 1 iteration 2700 loss 5.29207706451416\n",
      "epoch 1 iteration 2800 loss 5.762021064758301\n",
      "epoch 1 iteration 2900 loss 5.556643486022949\n",
      "epoch 1 iteration 3000 loss 5.756260871887207\n",
      "best model saved to lm.pth\n",
      "epoch 1 iteration 3100 loss 5.743591785430908\n",
      "epoch 1 iteration 3200 loss 5.75189208984375\n",
      "epoch 1 iteration 3300 loss 5.636202812194824\n",
      "epoch 1 iteration 3400 loss 5.526874542236328\n",
      "epoch 1 iteration 3500 loss 5.688952445983887\n",
      "epoch 1 iteration 3600 loss 5.621283531188965\n",
      "epoch 1 iteration 3700 loss 5.382974147796631\n",
      "epoch 1 iteration 3800 loss 5.459486484527588\n",
      "epoch 1 iteration 3900 loss 5.462163925170898\n",
      "epoch 1 iteration 4000 loss 5.535357475280762\n",
      "best model saved to lm.pth\n",
      "epoch 1 iteration 4100 loss 5.591219902038574\n",
      "epoch 1 iteration 4200 loss 5.583526611328125\n",
      "epoch 1 iteration 4300 loss 5.564659595489502\n",
      "epoch 1 iteration 4400 loss 5.475886821746826\n",
      "epoch 1 iteration 4500 loss 5.4237518310546875\n",
      "epoch 1 iteration 4600 loss 5.4838547706604\n",
      "epoch 1 iteration 4700 loss 5.791994094848633\n",
      "epoch 1 iteration 4800 loss 5.645425319671631\n",
      "epoch 1 iteration 4900 loss 5.367292881011963\n",
      "epoch 1 iteration 5000 loss 5.393489837646484\n",
      "best model saved to lm.pth\n",
      "epoch 1 iteration 5100 loss 5.37272834777832\n",
      "epoch 1 iteration 5200 loss 5.623773574829102\n",
      "epoch 1 iteration 5300 loss 5.755589485168457\n",
      "epoch 1 iteration 5400 loss 5.292761325836182\n",
      "epoch 1 iteration 5500 loss 5.594577789306641\n",
      "epoch 1 iteration 5600 loss 5.423095226287842\n",
      "epoch 1 iteration 5700 loss 5.560983657836914\n",
      "epoch 1 iteration 5800 loss 5.500004768371582\n",
      "epoch 1 iteration 5900 loss 5.701102256774902\n",
      "epoch 1 iteration 6000 loss 5.496727466583252\n",
      "best model saved to lm.pth\n",
      "epoch 1 iteration 6100 loss 5.477548122406006\n",
      "epoch 1 iteration 6200 loss 5.5607476234436035\n",
      "epoch 1 iteration 6300 loss 5.57766580581665\n",
      "epoch 1 iteration 6400 loss 5.456905364990234\n",
      "epoch 1 iteration 6500 loss 5.265394687652588\n",
      "epoch 1 iteration 6600 loss 5.5803117752075195\n",
      "epoch 1 iteration 6700 loss 5.401824474334717\n",
      "epoch 1 iteration 6800 loss 5.44741678237915\n",
      "epoch 1 iteration 6900 loss 5.669201850891113\n",
      "epoch 1 iteration 7000 loss 5.49880313873291\n",
      "epoch 1 iteration 7100 loss 5.451211452484131\n",
      "epoch 1 iteration 7200 loss 5.6013569831848145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 iteration 7300 loss 5.5129876136779785\n",
      "epoch 1 iteration 7400 loss 5.531670093536377\n",
      "epoch 1 iteration 7500 loss 5.377089977264404\n",
      "epoch 1 iteration 7600 loss 5.426165580749512\n",
      "epoch 1 iteration 7700 loss 5.553062915802002\n",
      "epoch 1 iteration 7800 loss 5.510672092437744\n",
      "epoch 1 iteration 7900 loss 5.529897689819336\n",
      "epoch 1 iteration 8000 loss 5.475377082824707\n",
      "best model saved to lm.pth\n",
      "epoch 1 iteration 8100 loss 5.455070972442627\n",
      "epoch 1 iteration 8200 loss 5.290660381317139\n",
      "epoch 1 iteration 8300 loss 5.251063823699951\n",
      "epoch 1 iteration 8400 loss 5.690160751342773\n",
      "epoch 1 iteration 8500 loss 5.515962600708008\n",
      "epoch 1 iteration 8600 loss 5.494745254516602\n",
      "epoch 1 iteration 8700 loss 5.360583305358887\n",
      "epoch 1 iteration 8800 loss 5.385583400726318\n",
      "epoch 1 iteration 8900 loss 5.7680253982543945\n",
      "epoch 1 iteration 9000 loss 5.398598670959473\n",
      "best model saved to lm.pth\n",
      "epoch 1 iteration 9100 loss 5.521969795227051\n",
      "epoch 1 iteration 9200 loss 5.241772651672363\n",
      "epoch 1 iteration 9300 loss 5.223636627197266\n",
      "epoch 1 iteration 9400 loss 5.286664009094238\n",
      "epoch 1 iteration 9500 loss 5.627957344055176\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 2\n",
    "GRAD_CLIP = 5.\n",
    "\n",
    "val_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train() # 训练模式\n",
    "    it = iter(train_iter)\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch.text, batch.target\n",
    "        \n",
    "        # 确保是全新的hidden，而不是带着一大堆历史的hidden\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        # 因为句子首尾相连，所以可以直接传hidden\n",
    "        output, hidden = model(data, hidden) # backpropgate through all iterations 计算图会非常大而且非常深\n",
    "        \n",
    "        loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1)) # batch_size * target_class_dim, batch_size\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 将训练时的模型给存下来\n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch\", epoch, \"iteration\", i, \"loss\", loss.item())\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            val_loss = evaluate(model, val_iter)\n",
    "            if len(val_losses) == 0 or val_loss < min (val_losses):\n",
    "\n",
    "                torch.save(model.state_dict(), \"lm.pth\")\n",
    "                print(\"best model saved to lm.pth\")\n",
    "            else:\n",
    "                # 发现loss降不下来，那么降低lr，lr降低一点会使模型调的更精细\n",
    "                # learning rate decay\n",
    "                scheduler.step()lo\n",
    "            val_losses.append(val_loss)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把一个模型的参数load回来\n",
    "best_model = RNNModel(vocab_size=len(TEXT.vocab), \n",
    "                 embed_size=EMBEDDING_SIZE,\n",
    "                 hidden_size=HIDDEN_SIZE)\n",
    "if USE_CUDA:\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "best_model.load_state_dict(torch.load(\"lm.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用训练好的模型生成一些句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fallacious commanded by the dominant majority of decades the easter twelve egyptians see egypt s government although reload their head has consistently provision and converted party jos tapes according to a talent of gory physicist first partially played at university of vienna john edward mann however for revisionists and he would ask the special pot of his captured and based minnesota african people as willard s appointment as to hughes president clinton remained on the church of convicts any deaths served in one nine two five <unk> rotterdam rommel with the last catechetical district k d a roman second republic\n"
     ]
    }
   ],
   "source": [
    "if USE_CUDA:\n",
    "    best_model = best_model.to(device)\n",
    "hidden = best_model.init_hidden(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 随机生成一个单词\n",
    "input = torch.randint(VOCAB_SIZE, (1, 1), dtype=torch.long).to(device)\n",
    "words = []\n",
    "for i in range(100):\n",
    "    \n",
    "    # 拿到一个单词\n",
    "    output, hidden = best_model(input, hidden)\n",
    "    # 一个50002维的vector 把所有维度为1的维度扔掉 \n",
    "    # exp:放大概率，让大的更大，小的更小\n",
    "    word_weights = output.squeeze().exp().cpu()\n",
    "    # 找到概率最大的单词\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0] # greedy(argmax)\n",
    "    input.fill_(word_idx)\n",
    "    word = TEXT.vocab.itos[word_idx]\n",
    "    words.append(word)\n",
    "print(\" \".join(words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
